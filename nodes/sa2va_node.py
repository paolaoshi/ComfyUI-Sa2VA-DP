# Sa2VA èŠ‚ç‚¹ for ComfyUI - å›¾åƒåˆ†å‰²å’Œç†è§£
# æ”¯æŒæ–‡æœ¬ç”Ÿæˆå’Œåˆ†å‰²æ©ç è¾“å‡º
# åŸºäº ByteDance/Sa2VA æ¨¡å‹ï¼Œç»“åˆ SAM2 å’Œ LLaVA

import torch
import numpy as np
import os
import gc
from contextlib import nullcontext
from PIL import Image
from typing import Tuple, List, Optional

# å¯¼å…¥æ¨¡å‹ç®¡ç†å™¨
import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥model_manager
current_dir = Path(__file__).parent.parent
sys.path.insert(0, str(current_dir))

from model_manager import get_model_manager


class Sa2VANode:
    """Sa2VA ComfyUIèŠ‚ç‚¹ - å›¾åƒåˆ†å‰²å’Œè§†è§‰ç†è§£"""
    
    def __init__(self):
        """åˆå§‹åŒ–èŠ‚ç‚¹"""
        self.model = None
        self.processor = None
        self.current_model_name = None  # è·Ÿè¸ªå½“å‰åŠ è½½çš„æ¨¡å‹
        self.model_manager = get_model_manager()  # è·å–æ¨¡å‹ç®¡ç†å™¨
    
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰èŠ‚ç‚¹çš„è¾“å…¥ç±»å‹"""
        return {
            "required": {
                # ğŸ–¼ï¸ å›¾åƒè¾“å…¥
                "ğŸ–¼ï¸å›¾åƒ": ("IMAGE",),
                
                # ğŸ¤– æ¨¡å‹é€‰æ‹©
                "ğŸ¤–æ¨¡å‹é€‰æ‹©": (
                    [
                        "Sa2VA-1B (1Bå‚æ•°)",
                        "Sa2VA-4B (4Bå‚æ•°)",
                        "Sa2VA-8B (8Bå‚æ•°)",
                        "Sa2VA-26B (26Bå‚æ•°)",
                        "Sa2VA-InternVL3-2B (2Bå‚æ•°)",
                        "Sa2VA-InternVL3-8B (8Bå‚æ•°)",
                        "Sa2VA-InternVL3-14B (14Bå‚æ•°)",
                        "Sa2VA-Qwen2.5-VL-3B (3Bå‚æ•°)",
                        "Sa2VA-Qwen2.5-VL-7B (7Bå‚æ•°)",
                        "Sa2VA-Qwen3-VL-4B (4Bå‚æ•°) â­æ¨è",
                    ],
                    {"default": "Sa2VA-Qwen3-VL-4B (4Bå‚æ•°) â­æ¨è"},
                ),
                
                # âš™ï¸ é‡åŒ–çº§åˆ«
                "âš™ï¸é‡åŒ–çº§åˆ«": (
                    [
                        "None (FP16/BF16)",
                        "4bit (NF4)",
                    ],
                    {"default": "None (FP16/BF16)"},
                ),
                
                # ğŸ’¬ æç¤ºè¯
                "ğŸ’¬æç¤ºè¯": (
                    "STRING",
                    {
                        "default": "è¯·æè¿°è¿™å¼ å›¾ç‰‡ï¼Œå¹¶ä¸ºç›¸åº”çš„éƒ¨åˆ†æä¾›åˆ†å‰²æ©ç ã€‚",
                        "multiline": True,
                    },
                ),
                
                # ğŸ­ é®ç½©é˜ˆå€¼
                "ğŸ­é®ç½©é˜ˆå€¼": (
                    "FLOAT",
                    {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.01},
                ),
                
                # ğŸ’» è®¾å¤‡é€‰æ‹©
                "ğŸ’»è®¾å¤‡é€‰æ‹©": (
                    ["auto", "cuda", "cpu"],
                    {"default": "auto"},
                ),
                
                # ğŸ² éšæœºç§å­
                "ğŸ²éšæœºç§å­": (
                    "INT",
                    {"default": 0, "min": 0, "max": 0xffffffffffffffff},
                ),
                
                # ğŸ¯ ç§å­æ§åˆ¶
                "ğŸ¯ç§å­æ§åˆ¶": (
                    ["å›ºå®š", "éšæœº", "é€’å¢"],
                    {"default": "å›ºå®š"},
                ),
            },
            "optional": {
                # âš¡ Flash Attention
                "âš¡å¯ç”¨FlashAttention": (
                    "BOOLEAN",
                    {"default": True},
                ),
                
                # ğŸ”„ ä¿æŒæ¨¡å‹åŠ è½½
                "ğŸ”„ä¿æŒæ¨¡å‹åŠ è½½": (
                    "BOOLEAN",
                    {"default": False},
                ),
                
                # ğŸ”ƒ å¼ºåˆ¶é‡æ–°ä¸‹è½½
                "ğŸ”ƒå¼ºåˆ¶é‡æ–°ä¸‹è½½": (
                    "BOOLEAN",
                    {"default": False},
                ),
                
                # ğŸ¨ é®ç½©é¢„å¤„ç†
                "ğŸ¨å¯ç”¨é®ç½©é¢„å¤„ç†": (
                    "BOOLEAN",
                    {"default": False},
                ),
                
                # ğŸ“ æ‰©å±•ï¼ˆåƒç´ ï¼‰
                "ğŸ“æ‰©å±•": (
                    "INT",
                    {"default": 0, "min": -999, "max": 999, "step": 1},
                ),
                
                # ğŸ“ æ‰©å±•å¢é‡
                "ğŸ“æ‰©å±•å¢é‡": (
                    "FLOAT",
                    {"default": 0.0, "min": 0.0, "max": 100.0, "step": 0.1},
                ),
                
                # ğŸ”² å€’è§’
                "ğŸ”²å€’è§’": (
                    "BOOLEAN",
                    {"default": True},
                ),
                
                # ğŸ”„ åè½¬è¾“å…¥
                "ğŸ”„åè½¬è¾“å…¥": (
                    "BOOLEAN",
                    {"default": False},
                ),
                
                # ğŸŒ«ï¸ æ¨¡ç³ŠåŠå¾„
                "ğŸŒ«ï¸æ¨¡ç³ŠåŠå¾„": (
                    "FLOAT",
                    {"default": 0.0, "min": 0.0, "max": 100.0, "step": 0.1},
                ),
                
                # ğŸ’« çº¿æ€§é€æ˜
                "ğŸ’«çº¿æ€§é€æ˜": (
                    "FLOAT",
                    {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01},
                ),
                
                # ğŸšï¸ è…èš€ç³»æ•°
                "ğŸšï¸è…èš€ç³»æ•°": (
                    "FLOAT",
                    {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01},
                ),
                
                # ğŸ”³ å¡«è¡¥
                "ğŸ”³å¡«è¡¥": (
                    "BOOLEAN",
                    {"default": False},
                ),
            },
        }
    
    RETURN_TYPES = ("STRING", "MASK", "IMAGE")
    RETURN_NAMES = ("ğŸ“Šç»“æœåˆ†æ", "åˆ†å‰²é®ç½©", "é®ç½©å›¾åƒ")
    FUNCTION = "process"
    CATEGORY = "ğŸ¤–å¤§ç‚®-Sa2VA"
    
    def check_dependencies(self) -> Tuple[bool, str]:
        """
        æ£€æŸ¥ä¾èµ–é¡¹æ˜¯å¦æ»¡è¶³
        
        Returns:
            (æ˜¯å¦æ»¡è¶³, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # æ£€æŸ¥transformersç‰ˆæœ¬
            from transformers import __version__ as transformers_version
            
            version_parts = transformers_version.split(".")
            major, minor = int(version_parts[0]), int(version_parts[1])
            
            # Sa2VAéœ€è¦transformers >= 4.57.0
            if major < 4 or (major == 4 and minor < 57):
                return (
                    False,
                    f"Sa2VAéœ€è¦ transformers >= 4.57.0ï¼Œå½“å‰ç‰ˆæœ¬: {transformers_version}\n"
                    f"è¯·è¿è¡Œ: pip install transformers>=4.57.0 --upgrade"
                )
            
            return True, transformers_version
            
        except Exception as e:
            return False, f"æ£€æŸ¥ä¾èµ–æ—¶å‡ºé”™: {e}"
    
    def load_model(
        self,
        model_name: str,
        quantization_level: str = "None (FP16/BF16)",
        device_choice: str = "auto",
        use_flash_attn: bool = True,
        force_download: bool = False,
        keep_model_loaded: bool = False,
    ) -> bool:
        """
        åŠ è½½Sa2VAæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆæ˜¾ç¤ºåç§°ï¼‰
            quantization_level: é‡åŒ–çº§åˆ«
            device_choice: è®¾å¤‡é€‰æ‹©
            use_flash_attn: æ˜¯å¦ä½¿ç”¨Flash Attention
            force_download: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
            keep_model_loaded: æ˜¯å¦ä¿æŒæ¨¡å‹åŠ è½½
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        # å¦‚æœæ¨¡å‹å·²åŠ è½½ä¸”æ˜¯åŒä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸”ä¿æŒåŠ è½½ï¼Œç›´æ¥è¿”å›
        if (
            keep_model_loaded
            and self.model is not None
            and self.processor is not None
            and self.current_model_name == model_name
        ):
            print(f"âœ… æ¨¡å‹å·²åŠ è½½ï¼ˆä¿æŒåŠ è½½æ¨¡å¼ï¼‰: {model_name}")
            return True
        
        # æ¸…ç†æ—§æ¨¡å‹
        if self.model is not None:
            try:
                del self.model
                self.model = None
            except:
                pass
        
        if self.processor is not None:
            try:
                del self.processor
                self.processor = None
            except:
                pass
        
        self.current_model_name = None
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            if hasattr(torch.cuda, "ipc_collect"):
                torch.cuda.ipc_collect()
        
        # è½¬æ¢æ˜¾ç¤ºåç§°ä¸ºå®é™…æ¨¡å‹åç§°
        model_name_map = {
            "Sa2VA-1B (1Bå‚æ•°)": "ByteDance/Sa2VA-1B",
            "Sa2VA-4B (4Bå‚æ•°)": "ByteDance/Sa2VA-4B",
            "Sa2VA-8B (8Bå‚æ•°)": "ByteDance/Sa2VA-8B",
            "Sa2VA-26B (26Bå‚æ•°)": "ByteDance/Sa2VA-26B",
            "Sa2VA-InternVL3-2B (2Bå‚æ•°)": "ByteDance/Sa2VA-InternVL3-2B",
            "Sa2VA-InternVL3-8B (8Bå‚æ•°)": "ByteDance/Sa2VA-InternVL3-8B",
            "Sa2VA-InternVL3-14B (14Bå‚æ•°)": "ByteDance/Sa2VA-InternVL3-14B",
            "Sa2VA-Qwen2.5-VL-3B (3Bå‚æ•°)": "ByteDance/Sa2VA-Qwen2_5-VL-3B",
            "Sa2VA-Qwen2.5-VL-7B (7Bå‚æ•°)": "ByteDance/Sa2VA-Qwen2_5-VL-7B",
            "Sa2VA-Qwen3-VL-4B (4Bå‚æ•°) â­æ¨è": "ByteDance/Sa2VA-Qwen3-VL-4B",
        }
        
        actual_model_name = model_name_map.get(model_name, model_name)
        print(f"ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹: {actual_model_name}")
        
        # æ£€æŸ¥ä¾èµ–
        deps_ok, deps_info = self.check_dependencies()
        if not deps_ok:
            print(f"âŒ {deps_info}")
            return False
        
        print(f"âœ… Transformersç‰ˆæœ¬æ£€æŸ¥é€šè¿‡: {deps_info}")
        
        try:
            # ä½¿ç”¨æ¨¡å‹ç®¡ç†å™¨ä¸‹è½½æ¨¡å‹
            success, model_path = self.model_manager.download_model(
                actual_model_name, 
                force_download=force_download
            )
            
            if not success:
                print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_path}")
                return False
            
            print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
            
            # å¯¼å…¥transformers
            from transformers import AutoProcessor, AutoModel
            
            # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = {
                "low_cpu_mem_usage": True,
                "trust_remote_code": True,
            }
            
            # é‡åŒ–é…ç½®
            use_quantization = quantization_level != "None (FP16/BF16)"
            if use_quantization:
                try:
                    from transformers import BitsAndBytesConfig
                    
                    if quantization_level == "4bit (NF4)":
                        quantization_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_compute_dtype=torch.bfloat16,
                            bnb_4bit_use_double_quant=True,
                            bnb_4bit_quant_type="nf4",
                        )
                        print("âœ… å¯ç”¨4ä½é‡åŒ– (NF4)")
                    
                    model_kwargs["quantization_config"] = quantization_config
                except ImportError:
                    print("âš ï¸ bitsandbytesæœªå®‰è£…ï¼Œè·³è¿‡é‡åŒ–")
                    print("   å®‰è£…å‘½ä»¤: pip install bitsandbytes")
                    use_quantization = False
            
            # Flash Attentioné…ç½®
            if use_flash_attn:
                try:
                    import flash_attn
                    model_kwargs["use_flash_attn"] = True
                    print("âœ… å¯ç”¨Flash Attention")
                except ImportError:
                    print("âš ï¸ flash-attnæœªå®‰è£…ï¼Œè·³è¿‡Flash Attention")
                    print("   å®‰è£…å‘½ä»¤: pip install flash-attn")
            
            # ç¡®å®šç›®æ ‡è®¾å¤‡
            if device_choice == "auto":
                target_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            elif device_choice == "cuda":
                if torch.cuda.is_available():
                    target_device = torch.device("cuda")
                else:
                    print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
                    target_device = torch.device("cpu")
            else:
                target_device = torch.device("cpu")
            
            print(f"ğŸ’» ç›®æ ‡è®¾å¤‡: {target_device}")
            
            # è®¾ç½®æ•°æ®ç±»å‹
            if not use_quantization:
                if target_device.type == "cuda":
                    # ä¼˜å…ˆä½¿ç”¨bfloat16ï¼Œå¦‚æœä¸æ”¯æŒåˆ™ä½¿ç”¨float16
                    if hasattr(torch.cuda, "is_bf16_supported") and torch.cuda.is_bf16_supported():
                        model_kwargs["torch_dtype"] = torch.bfloat16
                        print("âœ… ä½¿ç”¨bfloat16ç²¾åº¦")
                    else:
                        model_kwargs["torch_dtype"] = torch.float16
                        print("âœ… ä½¿ç”¨float16ç²¾åº¦")
                else:
                    model_kwargs["torch_dtype"] = torch.float32
                    print("âœ… ä½¿ç”¨float32ç²¾åº¦ï¼ˆCPUæ¨¡å¼ï¼‰")
            
            # ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹
            print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡...")
            self.model = AutoModel.from_pretrained(
                model_path,
                **model_kwargs
            ).eval()
            
            # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
            if not use_quantization:  # é‡åŒ–ä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡
                self.model = self.model.to(target_device)
                print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°: {target_device}")
            
            # åŠ è½½å¤„ç†å™¨
            print("ğŸ”„ æ­£åœ¨åŠ è½½å¤„ç†å™¨...")
            self.processor = AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=True,
                use_fast=False,
            )
            
            self.current_model_name = model_name
            
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def process_image(
        self,
        image: Image.Image,
        prompt: str,
    ) -> Tuple[str, List]:
        """
        å¤„ç†å•å¼ å›¾åƒ
        
        Args:
            image: PILå›¾åƒ
            prompt: æç¤ºè¯
        
        Returns:
            (æ–‡æœ¬è¾“å‡º, åˆ†å‰²æ©ç åˆ—è¡¨)
        """
        try:
            # å‡†å¤‡è¾“å…¥
            input_dict = {
                "image": image,
                "text": f"<image>{prompt}",
                "past_text": "",
                "mask_prompts": None,
                "processor": self.processor,
            }
            
            # æ¨ç†
            with torch.no_grad():
                return_dict = self.model.predict_forward(**input_dict)
            
            # æå–ç»“æœ
            text_output = return_dict.get("prediction", "")
            masks = return_dict.get("prediction_masks", [])
            
            return text_output, masks
            
        except Exception as e:
            error_msg = f"å¤„ç†å›¾åƒæ—¶å‡ºé”™: {e}"
            print(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            return error_msg, []
    
    def _generate_analysis_report(
        self,
        model_name: str,
        quantization_level: str,
        device_choice: str,
        image_size: tuple,
        num_masks: int,
        mask_threshold: float,
        process_time: float,
        total_time: float,
        seed: int,
        seed_control: str,
        model_output: str
    ) -> str:
        """
        ç”Ÿæˆè¯¦ç»†çš„ç»“æœåˆ†ææŠ¥å‘Š
        
        Args:
            model_name: æ¨¡å‹åç§°
            quantization_level: é‡åŒ–çº§åˆ«
            device_choice: è®¾å¤‡é€‰æ‹©
            image_size: å›¾åƒå°ºå¯¸ (width, height)
            num_masks: æ£€æµ‹åˆ°çš„æ©ç æ•°é‡
            mask_threshold: æ©ç é˜ˆå€¼
            process_time: æ¨¡å‹å¤„ç†æ—¶é—´
            total_time: æ€»å¤„ç†æ—¶é—´
            seed: éšæœºç§å­
            seed_control: ç§å­æ§åˆ¶æ¨¡å¼
            model_output: æ¨¡å‹æ–‡æœ¬è¾“å‡º
        
        Returns:
            æ ¼å¼åŒ–çš„åˆ†ææŠ¥å‘Š
        """
        w, h = image_size
        
        # è·å–è®¾å¤‡ä¿¡æ¯
        if torch.cuda.is_available():
            device_name = torch.cuda.get_device_name(0)
            device_info = f"CUDA ({device_name})"
        else:
            device_info = "CPU"
        
        # æ„å»ºæŠ¥å‘Š
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   Sa2VA æ‰§è¡Œç»“æœåˆ†æ                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… æ‰§è¡ŒçŠ¶æ€: æˆåŠŸå®Œæˆ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š æ‰§è¡Œé…ç½®
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}
âš™ï¸ é‡åŒ–çº§åˆ«: {quantization_level}
ğŸ’» è¿è¡Œè®¾å¤‡: {device_info} (é€‰æ‹©: {device_choice})
ğŸ“ å›¾åƒå°ºå¯¸: {w} Ã— {h} åƒç´ 

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ åˆ†å‰²ç»“æœ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ­ æ©ç é˜ˆå€¼: {mask_threshold}
   è¯´æ˜: æ©ç é˜ˆå€¼ç”¨äºå°†æ¨¡å‹è¾“å‡ºçš„è¿ç»­å€¼æ©ç è½¬æ¢ä¸ºäºŒå€¼æ©ç 
         - é˜ˆå€¼èŒƒå›´: 0.0 åˆ° 1.0
         - åƒç´ å€¼ > {mask_threshold} â†’ å‰æ™¯ (ç™½è‰², å€¼=1)
         - åƒç´ å€¼ â‰¤ {mask_threshold} â†’ èƒŒæ™¯ (é»‘è‰², å€¼=0)
         - é˜ˆå€¼è¶Šé«˜ï¼Œåˆ†å‰²è¶Šä¸¥æ ¼ï¼Œä¿ç•™çš„åŒºåŸŸè¶Šå°‘
         - é˜ˆå€¼è¶Šä½ï¼Œåˆ†å‰²è¶Šå®½æ¾ï¼Œä¿ç•™çš„åŒºåŸŸè¶Šå¤š
         - æ¨èå€¼: 0.5 (é»˜è®¤)

âœ… æ£€æµ‹åˆ°æ©ç æ•°é‡: {num_masks} ä¸ª
   - æ¯ä¸ªæ©ç å¯¹åº”æ¨¡å‹è¯†åˆ«çš„ä¸€ä¸ªç‰©ä½“æˆ–åŒºåŸŸ
   - æ©ç å·²æ ¹æ®é˜ˆå€¼ {mask_threshold} è¿›è¡ŒäºŒå€¼åŒ–å¤„ç†

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸ æ€§èƒ½ç»Ÿè®¡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”„ æ¨¡å‹æ¨ç†æ—¶é—´: {process_time:.2f} ç§’
ğŸ“¦ æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’
âš¡ å¤„ç†é€Ÿåº¦: {1/total_time:.2f} å¼ /ç§’

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ² éšæœºç§å­ä¿¡æ¯
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ ç§å­æ§åˆ¶: {seed_control}
ğŸ² ä½¿ç”¨ç§å­: {seed}
   è¯´æ˜: ä½¿ç”¨ç›¸åŒçš„ç§å­å’Œå‚æ•°å¯ä»¥é‡ç°ç›¸åŒçš„ç»“æœ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¬ æ¨¡å‹è¾“å‡ºæ‘˜è¦
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{model_output[:200]}{"..." if len(model_output) > 200 else ""}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ¨ è¾“å‡ºè¯´æ˜
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š ç»“æœåˆ†æ: æœ¬æŠ¥å‘Šï¼ˆå½“å‰è¾“å‡ºï¼‰
ğŸ­ åˆ†å‰²æ©ç : äºŒå€¼åŒ–æ©ç å¼ é‡ï¼Œå¯ç”¨äºåç»­å¤„ç†
ğŸ–¼ï¸ æ©ç å›¾åƒ: å¯è§†åŒ–çš„æ©ç å›¾åƒï¼Œå¯ç›´æ¥é¢„è§ˆ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        return report.strip()
    
    def preprocess_mask(
        self,
        mask: torch.Tensor,
        expand: int = 0,
        incremental_expand: float = 0.0,
        tapered_corners: bool = True,
        invert_input: bool = False,
        blur_radius: float = 0.0,
        lerp_alpha: float = 1.0,
        decay_factor: float = 1.0,
        fill_holes: bool = False,
    ) -> torch.Tensor:
        """
        é®ç½©é¢„å¤„ç†å‡½æ•°ï¼ˆå‚è€ƒKJNodesçš„é®ç½©æ¨¡ç³Šç”Ÿé•¿ï¼‰
        
        Args:
            mask: è¾“å…¥é®ç½© (H, W)
            expand: æ‰©å±•åƒç´ æ•°
            incremental_expand: æ‰©å±•å¢é‡
            tapered_corners: æ˜¯å¦å€’è§’
            invert_input: æ˜¯å¦åè½¬è¾“å…¥
            blur_radius: æ¨¡ç³ŠåŠå¾„
            lerp_alpha: çº¿æ€§é€æ˜åº¦
            decay_factor: è…èš€ç³»æ•°
            fill_holes: æ˜¯å¦å¡«è¡¥å­”æ´
        
        Returns:
            å¤„ç†åçš„é®ç½©
        """
        try:
            import cv2
            from scipy.ndimage import binary_fill_holes, distance_transform_edt
            
            # è½¬æ¢ä¸ºnumpy
            if isinstance(mask, torch.Tensor):
                mask_np = mask.detach().cpu().numpy()
            else:
                mask_np = mask.copy()
            
            # ç¡®ä¿æ˜¯2D
            if len(mask_np.shape) == 3:
                mask_np = mask_np[0] if mask_np.shape[0] == 1 else mask_np[:, :, 0]
            
            # åè½¬è¾“å…¥
            if invert_input:
                mask_np = 1.0 - mask_np
            
            # å¡«è¡¥å­”æ´
            if fill_holes:
                mask_bool = mask_np > 0.5
                mask_filled = binary_fill_holes(mask_bool)
                mask_np = mask_filled.astype(np.float32)
            
            # æ‰©å±•/è…èš€
            total_expand = expand + incremental_expand
            if abs(total_expand) > 0:
                kernel_size = int(abs(total_expand) * 2) + 1
                kernel = cv2.getStructuringElement(
                    cv2.MORPH_ELLIPSE if tapered_corners else cv2.MORPH_RECT,
                    (kernel_size, kernel_size)
                )
                
                if total_expand > 0:
                    # è†¨èƒ€
                    mask_np = cv2.dilate(mask_np, kernel, iterations=1)
                else:
                    # è…èš€
                    mask_np = cv2.erode(mask_np, kernel, iterations=1)
            
            # åº”ç”¨è…èš€ç³»æ•°
            if decay_factor < 1.0:
                # è®¡ç®—è·ç¦»å˜æ¢
                binary_mask = (mask_np > 0.5).astype(np.uint8)
                dist_transform = distance_transform_edt(binary_mask)
                
                # å½’ä¸€åŒ–è·ç¦»
                if dist_transform.max() > 0:
                    dist_norm = dist_transform / dist_transform.max()
                    # åº”ç”¨è¡°å‡
                    mask_np = mask_np * (dist_norm ** (1.0 - decay_factor))
            
            # æ¨¡ç³Š
            if blur_radius > 0:
                kernel_size = int(blur_radius * 2) + 1
                if kernel_size % 2 == 0:
                    kernel_size += 1
                mask_np = cv2.GaussianBlur(mask_np, (kernel_size, kernel_size), blur_radius / 2)
            
            # çº¿æ€§æ’å€¼ï¼ˆé€æ˜åº¦ï¼‰
            if lerp_alpha < 1.0:
                original = mask.detach().cpu().numpy() if isinstance(mask, torch.Tensor) else mask
                if len(original.shape) == 3:
                    original = original[0] if original.shape[0] == 1 else original[:, :, 0]
                mask_np = original * (1.0 - lerp_alpha) + mask_np * lerp_alpha
            
            # ç¡®ä¿åœ¨0-1èŒƒå›´
            mask_np = np.clip(mask_np, 0.0, 1.0)
            
            # è½¬å›torch
            return torch.from_numpy(mask_np).float()
            
        except Exception as e:
            print(f"âš ï¸ é®ç½©é¢„å¤„ç†å¤±è´¥: {e}")
            return mask
    
    def convert_masks_to_comfyui(
        self,
        masks: List,
        height: int,
        width: int,
        threshold: float = 0.5,
        enable_preprocess: bool = False,
        expand: int = 0,
        incremental_expand: float = 0.0,
        tapered_corners: bool = True,
        invert_input: bool = False,
        blur_radius: float = 0.0,
        lerp_alpha: float = 1.0,
        decay_factor: float = 1.0,
        fill_holes: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å°†Sa2VAçš„æ©ç è½¬æ¢ä¸ºComfyUIæ ¼å¼
        
        Args:
            masks: Sa2VAè¾“å‡ºçš„æ©ç åˆ—è¡¨
            height: å›¾åƒé«˜åº¦
            width: å›¾åƒå®½åº¦
            threshold: äºŒå€¼åŒ–é˜ˆå€¼
        
        Returns:
            (æ©ç å¼ é‡, æ©ç å›¾åƒå¼ é‡)
        """
        try:
            # å¦‚æœæ²¡æœ‰æ©ç ï¼Œè¿”å›ç©ºæ©ç 
            if masks is None or len(masks) == 0:
                print("âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°æ©ç ï¼Œè¿”å›ç©ºæ©ç ")
                empty_mask = torch.zeros((1, height, width), dtype=torch.float32)
                empty_image = torch.zeros((1, height, width, 3), dtype=torch.float32)
                return empty_mask, empty_image
            
            comfyui_masks = []
            mask_images = []
            
            for i, mask in enumerate(masks):
                if mask is None:
                    continue
                
                try:
                    # è½¬æ¢ä¸ºnumpyæ•°ç»„
                    if isinstance(mask, torch.Tensor):
                        mask_np = mask.detach().cpu().numpy()
                    elif isinstance(mask, np.ndarray):
                        mask_np = mask.copy()
                    else:
                        continue
                    
                    # å¤„ç†ä¸åŒçš„ç»´åº¦
                    if len(mask_np.shape) == 4:  # (batch, channel, height, width)
                        mask_np = mask_np[0, 0]
                    elif len(mask_np.shape) == 3:
                        if mask_np.shape[0] == 1:  # (1, height, width)
                            mask_np = mask_np[0]
                        elif mask_np.shape[2] == 1:  # (height, width, 1)
                            mask_np = mask_np[:, :, 0]
                        else:
                            mask_np = mask_np[0]
                    
                    # ç¡®ä¿æ˜¯2Dæ©ç 
                    if len(mask_np.shape) != 2:
                        continue
                    
                    # è½¬æ¢ä¸ºfloat32
                    if mask_np.dtype == bool:
                        mask_np = mask_np.astype(np.float32)
                    elif not np.issubdtype(mask_np.dtype, np.floating):
                        mask_np = mask_np.astype(np.float32)
                    
                    # å¤„ç†NaNå’Œæ— ç©·å€¼
                    if np.any(np.isnan(mask_np)) or np.any(np.isinf(mask_np)):
                        mask_np = np.nan_to_num(mask_np, nan=0.0, posinf=1.0, neginf=0.0)
                    
                    # å½’ä¸€åŒ–åˆ°0-1
                    mask_min, mask_max = mask_np.min(), mask_np.max()
                    if mask_max > mask_min:
                        mask_np = (mask_np - mask_min) / (mask_max - mask_min)
                    else:
                        mask_np = np.ones_like(mask_np) if mask_min > 0 else np.zeros_like(mask_np)
                    
                    # åº”ç”¨é˜ˆå€¼
                    mask_np = (mask_np > threshold).astype(np.float32)
                    
                    # è½¬æ¢ä¸ºtorchå¼ é‡
                    mask_tensor = torch.from_numpy(mask_np).float()
                    
                    # åº”ç”¨é¢„å¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if enable_preprocess:
                        mask_tensor = self.preprocess_mask(
                            mask_tensor,
                            expand=expand,
                            incremental_expand=incremental_expand,
                            tapered_corners=tapered_corners,
                            invert_input=invert_input,
                            blur_radius=blur_radius,
                            lerp_alpha=lerp_alpha,
                            decay_factor=decay_factor,
                            fill_holes=fill_holes,
                        )
                    
                    comfyui_masks.append(mask_tensor)
                    
                    # åˆ›å»ºRGBæ©ç å›¾åƒ
                    rgb_np = np.stack([mask_np, mask_np, mask_np], axis=-1)
                    rgb_np = np.clip(rgb_np, 0.0, 1.0).astype(np.float32)
                    mask_images.append(torch.from_numpy(rgb_np))
                    
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†ç¬¬{i}ä¸ªæ©ç æ—¶å‡ºé”™: {e}")
                    continue
            
            # å¦‚æœæ²¡æœ‰æˆåŠŸå¤„ç†çš„æ©ç 
            if not comfyui_masks:
                empty_mask = torch.zeros((1, height, width), dtype=torch.float32)
                empty_image = torch.zeros((1, height, width, 3), dtype=torch.float32)
                return empty_mask, empty_image
            
            # å †å æ©ç 
            final_masks = torch.stack(comfyui_masks, dim=0)  # (N, H, W)
            final_images = torch.stack(mask_images, dim=0)   # (N, H, W, 3)
            
            return final_masks, final_images
            
        except Exception as e:
            print(f"âŒ è½¬æ¢æ©ç æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            empty_mask = torch.zeros((1, height, width), dtype=torch.float32)
            empty_image = torch.zeros((1, height, width, 3), dtype=torch.float32)
            return empty_mask, empty_image
    
    def process(
        self,
        **kwargs
    ) -> Tuple[str, torch.Tensor, torch.Tensor]:
        """
        ä¸»å¤„ç†å‡½æ•°
        
        Args:
            **kwargs: æ‰€æœ‰è¾“å…¥å‚æ•°
        
        Returns:
            (æ–‡æœ¬è¾“å‡º, æ©ç å¼ é‡, æ©ç å›¾åƒå¼ é‡)
        """
        try:
            # æå–å‚æ•°ï¼ˆä½¿ç”¨ä¸­æ–‡é”®åï¼‰
            image = kwargs.get("ğŸ–¼ï¸å›¾åƒ")
            model_name = kwargs.get("ğŸ¤–æ¨¡å‹é€‰æ‹©")
            quantization_level = kwargs.get("âš™ï¸é‡åŒ–çº§åˆ«", "None (FP16/BF16)")
            prompt = kwargs.get("ğŸ’¬æç¤ºè¯")
            mask_threshold = kwargs.get("ğŸ­é®ç½©é˜ˆå€¼", 0.5)
            device_choice = kwargs.get("ğŸ’»è®¾å¤‡é€‰æ‹©", "auto")
            seed = kwargs.get("ğŸ²éšæœºç§å­", 0)
            seed_control = kwargs.get("ğŸ¯ç§å­æ§åˆ¶", "å›ºå®š")
            use_flash_attn = kwargs.get("âš¡å¯ç”¨FlashAttention", True)
            keep_model_loaded = kwargs.get("ğŸ”„ä¿æŒæ¨¡å‹åŠ è½½", False)
            force_download = kwargs.get("ğŸ”ƒå¼ºåˆ¶é‡æ–°ä¸‹è½½", False)
            
            # é®ç½©é¢„å¤„ç†å‚æ•°
            enable_preprocess = kwargs.get("ğŸ¨å¯ç”¨é®ç½©é¢„å¤„ç†", False)
            expand = kwargs.get("ğŸ“æ‰©å±•", 0)
            incremental_expand = kwargs.get("ğŸ“æ‰©å±•å¢é‡", 0.0)
            tapered_corners = kwargs.get("ğŸ”²å€’è§’", True)
            invert_input = kwargs.get("ğŸ”„åè½¬è¾“å…¥", False)
            blur_radius = kwargs.get("ğŸŒ«ï¸æ¨¡ç³ŠåŠå¾„", 0.0)
            lerp_alpha = kwargs.get("ğŸ’«çº¿æ€§é€æ˜", 1.0)
            decay_factor = kwargs.get("ğŸšï¸è…èš€ç³»æ•°", 1.0)
            fill_holes = kwargs.get("ğŸ”³å¡«è¡¥", False)
            
            # å¤„ç†éšæœºç§å­
            if seed_control == "éšæœº":
                import random
                seed = random.randint(0, 0xffffffffffffffff)
                print(f"ğŸ² ä½¿ç”¨éšæœºç§å­: {seed}")
            elif seed_control == "é€’å¢":
                if not hasattr(self, '_last_seed'):
                    self._last_seed = seed
                else:
                    self._last_seed += 1
                seed = self._last_seed
                print(f"ğŸ² ä½¿ç”¨é€’å¢ç§å­: {seed}")
            else:
                print(f"ğŸ² ä½¿ç”¨å›ºå®šç§å­: {seed}")
            
            # è®¾ç½®éšæœºç§å­
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(seed)
            np.random.seed(seed % (2**32))
            
            # åŠ è½½æ¨¡å‹
            if not self.load_model(
                model_name, 
                quantization_level,
                device_choice,
                use_flash_attn, 
                force_download,
                keep_model_loaded
            ):
                error_msg = f"æ¨¡å‹åŠ è½½å¤±è´¥: {model_name}"
                print(f"âŒ {error_msg}")
                # è¿”å›é”™è¯¯ä¿¡æ¯å’Œç©ºæ©ç 
                h, w = 512, 512
                if hasattr(image, "shape") and len(image.shape) >= 2:
                    if len(image.shape) == 4:
                        h, w = image.shape[1], image.shape[2]
                    elif len(image.shape) == 3:
                        h, w = image.shape[0], image.shape[1]
                empty_mask = torch.zeros((1, h, w), dtype=torch.float32)
                empty_image = torch.zeros((1, h, w, 3), dtype=torch.float32)
                return error_msg, empty_mask, empty_image
            
            # éªŒè¯è¾“å…¥
            if image is None:
                error_msg = "æœªæä¾›å›¾åƒ"
                print(f"âš ï¸ {error_msg}")
                empty_mask = torch.zeros((1, 512, 512), dtype=torch.float32)
                empty_image = torch.zeros((1, 512, 512, 3), dtype=torch.float32)
                return error_msg, empty_mask, empty_image
            
            print(f"ğŸ”„ å¼€å§‹å¤„ç†å›¾åƒ...")
            
            # è½¬æ¢ComfyUIå›¾åƒä¸ºPILå›¾åƒ
            if hasattr(image, "shape") and len(image.shape) == 4:
                # ComfyUIæ ¼å¼: (batch, height, width, channels)
                img_tensor = image[0]
            elif hasattr(image, "shape") and len(image.shape) == 3:
                # å•å¼ å›¾åƒ: (height, width, channels)
                img_tensor = image
            else:
                error_msg = f"ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼: {type(image)}"
                print(f"âŒ {error_msg}")
                empty_mask = torch.zeros((1, 512, 512), dtype=torch.float32)
                empty_image = torch.zeros((1, 512, 512, 3), dtype=torch.float32)
                return error_msg, empty_mask, empty_image
            
            # è½¬æ¢ä¸ºnumpy
            if isinstance(img_tensor, torch.Tensor):
                img_tensor = img_tensor.detach().cpu()
                image_np = img_tensor.numpy()
            else:
                error_msg = f"ä¸æ”¯æŒçš„å¼ é‡ç±»å‹: {type(image)}"
                print(f"âŒ {error_msg}")
                empty_mask = torch.zeros((1, 512, 512), dtype=torch.float32)
                empty_image = torch.zeros((1, 512, 512, 3), dtype=torch.float32)
                return error_msg, empty_mask, empty_image
            
            # è½¬æ¢ä¸ºuint8
            if image_np.dtype != "uint8":
                image_np = (image_np * 255).astype("uint8")
            
            # è½¬æ¢ä¸ºPILå›¾åƒ
            pil_image = Image.fromarray(image_np)
            h, w = image_np.shape[0], image_np.shape[1]
            
            print(f"ğŸ“ å›¾åƒå°ºå¯¸: {w}x{h}")
            
            # è®°å½•å¼€å§‹æ—¶é—´
            import time
            start_time = time.time()
            
            # å¤„ç†å›¾åƒ
            with torch.inference_mode():
                text_output, masks = self.process_image(pil_image, prompt)
            
            # è®°å½•å¤„ç†æ—¶é—´
            process_time = time.time() - start_time
            
            print(f"âœ… æ¨¡å‹è¾“å‡º: {text_output[:100]}...")  # åªæ‰“å°å‰100ä¸ªå­—ç¬¦
            print(f"âœ… æ£€æµ‹åˆ° {len(masks)} ä¸ªæ©ç ")
            
            # è½¬æ¢æ©ç 
            comfyui_masks, mask_images = self.convert_masks_to_comfyui(
                masks, h, w, mask_threshold,
                enable_preprocess=enable_preprocess,
                expand=expand,
                incremental_expand=incremental_expand,
                tapered_corners=tapered_corners,
                invert_input=invert_input,
                blur_radius=blur_radius,
                lerp_alpha=lerp_alpha,
                decay_factor=decay_factor,
                fill_holes=fill_holes,
            )
            
            # è®¡ç®—æ€»æ—¶é—´
            total_time = time.time() - start_time
            
            print(f"âœ… å¤„ç†å®Œæˆ")
            print(f"   æ©ç å½¢çŠ¶: {comfyui_masks.shape}")
            print(f"   æ©ç å›¾åƒå½¢çŠ¶: {mask_images.shape}")
            
            # ç”Ÿæˆè¯¦ç»†çš„ç»“æœåˆ†ææŠ¥å‘Š
            analysis_report = self._generate_analysis_report(
                model_name=model_name,
                quantization_level=quantization_level,
                device_choice=device_choice,
                image_size=(w, h),
                num_masks=len(masks),
                mask_threshold=mask_threshold,
                process_time=process_time,
                total_time=total_time,
                seed=seed,
                seed_control=seed_control,
                model_output=text_output
            )
            
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            return analysis_report, comfyui_masks, mask_images
            
        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            
            # è¿”å›é”™è¯¯å’Œç©ºæ©ç 
            h, w = 512, 512
            try:
                if hasattr(image, "shape") and len(image.shape) >= 2:
                    if len(image.shape) == 4:
                        h, w = image.shape[1], image.shape[2]
                    elif len(image.shape) == 3:
                        h, w = image.shape[0], image.shape[1]
            except:
                pass
            
            empty_mask = torch.zeros((1, h, w), dtype=torch.float32)
            empty_image = torch.zeros((1, h, w, 3), dtype=torch.float32)
            return f"é”™è¯¯: {error_msg}", empty_mask, empty_image
